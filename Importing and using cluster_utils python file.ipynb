{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f19a22",
   "metadata": {},
   "source": [
    "# Path Config & Global Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a70a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 0) Environment Setup MUST come first\n",
    "# ======================================================\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"   # MUST be before numpy/sklearn imports\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 1) Imports & Global Config\n",
    "# ======================================================\n",
    "# Standard library\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Numerical / Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# ML / NLP\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from minisom import MiniSom\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# Reproducibility\n",
    "GLOBAL_SEED = 42\n",
    "random.seed(GLOBAL_SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(GLOBAL_SEED)\n",
    "\n",
    "# Project path\n",
    "BASE_PATH = Path.cwd()\n",
    "print(\"Working in:\", BASE_PATH)\n",
    "\n",
    "# Warnings cleanup\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Load clustering utilities\n",
    "import cluster_utils as cu\n",
    "print(\"Imported cluster_utils.py successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd5dd43",
   "metadata": {},
   "source": [
    "# Load Data and Create Trace Strings for Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a078e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# Vectorization & Clustering Pipeline (Annotated Version)\n",
    "# ======================================================\n",
    "# This script loads event-log data, constructs textual variants of cases,\n",
    "# encodes them using TF-IDF and Doc2Vec, reduces dimensionality when needed,\n",
    "# and runs several clustering algorithms (KMeans, SOM, HDBSCAN). Metrics and\n",
    "# cluster-level summaries are also computed.\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 2) Load Event Log Data\n",
    "# ======================================================\n",
    "\n",
    "log_path = BASE_PATH / \"df_sampled_200_cases.csv\"\n",
    "\n",
    "df = pd.read_csv(log_path)\n",
    "\n",
    "# Parse timestamps\n",
    "df[\"time:timestamp\"] = pd.to_datetime(df[\"time:timestamp\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"time:timestamp\"])\n",
    "df = df.sort_values([\"case:concept:name\", \"time:timestamp\"]).copy()\n",
    "\n",
    "print(\"Log loaded with\", len(df), \"events.\")\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f1bebc",
   "metadata": {},
   "source": [
    "## Extract Trace Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd06b645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 3) Extract trace sequences ‚Üí FILTERED\n",
    "# ======================================================\n",
    "\n",
    "filtered = cu.extract_trace_sequences(\n",
    "    df,\n",
    "    case_col=\"case:concept:name\",\n",
    "    activity_col=\"concept:name\",\n",
    "    timestamp_col=\"time:timestamp\",\n",
    "    min_variant_freq=2\n",
    ")\n",
    "\n",
    "print(\"Extracted\", len(filtered), \"trace sequences.\")\n",
    "\n",
    "# PM4Py downstream requires case IDs as index\n",
    "filtered = filtered.set_index(\"case:concept:name\")\n",
    "filtered.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eafc796",
   "metadata": {},
   "source": [
    "## Vectorize trace sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e427021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 4) Vectorization (TF-IDF + SVD + Doc2Vec)\n",
    "# ======================================================\n",
    "\n",
    "encoders, artifacts = cu.vectorize(\n",
    "    filtered[\"trace_str\"],   # <‚Äî FIXED: was seqs\n",
    "    method=\"both\"\n",
    ")\n",
    "\n",
    "for name, X in encoders.items():\n",
    "    print(name, X.shape)\n",
    "\n",
    "X = encoders[\"TFIDF_SVD\"]\n",
    "print(\"Using TFIDF_SVD:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fd340d",
   "metadata": {},
   "source": [
    "## TFIDF Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f2edc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 5) KMeans Sweep ‚Üí Best K\n",
    "# ======================================================\n",
    "\n",
    "K_RANGE = range(2, 16)\n",
    "\n",
    "best_km, km_results = cu.find_num_clusters(\n",
    "    X,\n",
    "    K_RANGE,\n",
    "    cluster_algo=\"kmeans\"\n",
    ")\n",
    "\n",
    "best_km\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7681fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot silhouette curve\n",
    "cu.plot_silhouette_trend(km_results, \"KMeans Silhouette Trend (TF-IDF)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130c832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final KMeans clustering\n",
    "chosen_k = best_km[\"k\"]\n",
    "\n",
    "labels, model = cu.cluster_traces(\n",
    "    X,\n",
    "    num_clusters=chosen_k,\n",
    "    cluster_algo=\"kmeans\"\n",
    ")\n",
    "\n",
    "print(\"Assigned clusters:\", set(labels))\n",
    "\n",
    "cluster_stats = cu.mine_from_clusters(\n",
    "    labels,\n",
    "    num_clusters=chosen_k,\n",
    "    sequences_df=filtered.reset_index()   # <‚Äî FIXED\n",
    ")\n",
    "\n",
    "cluster_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7b9e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 6) SOM Sweep ‚Üí Best K\n",
    "# ======================================================\n",
    "\n",
    "best_som, som_results = cu.find_num_clusters(\n",
    "    X,\n",
    "    K_RANGE,\n",
    "    cluster_algo=\"som\"\n",
    ")\n",
    "\n",
    "cu.plot_silhouette_trend(som_results, \"SOM Silhouette Trend (TF-IDF)\")\n",
    "\n",
    "labels_som, _ = cu.cluster_traces(\n",
    "    X,\n",
    "    num_clusters=best_som[\"k\"],\n",
    "    cluster_algo=\"som\"\n",
    ")\n",
    "\n",
    "cu.mine_from_clusters(\n",
    "    labels_som,\n",
    "    num_clusters=best_som[\"k\"],\n",
    "    sequences_df=filtered.reset_index()   # <‚Äî FIXED\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0585fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 7) HDBSCAN (no K needed)\n",
    "# ======================================================\n",
    "\n",
    "labels_hdbscan, hdb_model = cu.cluster_traces(\n",
    "    X,\n",
    "    cluster_algo=\"hdbscan\"\n",
    ")\n",
    "\n",
    "cluster_stats_hdb = cu.mine_from_clusters(\n",
    "    labels_hdbscan,\n",
    "    num_clusters=None,\n",
    "    sequences_df=filtered.reset_index()   # <‚Äî FIXED\n",
    ")\n",
    "\n",
    "cluster_stats_hdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb659d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 8) Attach Cluster Assignments to FILTERED\n",
    "# ======================================================\n",
    "\n",
    "filtered[\"cluster_kmeans_tfidf_svd\"] = labels\n",
    "filtered[\"cluster_som_tfidf_svd\"]     = labels_som\n",
    "filtered[\"cluster_hdbscan_tfidf_svd\"] = labels_hdbscan\n",
    "\n",
    "filtered.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b59bcb",
   "metadata": {},
   "source": [
    "# Doc2Vec Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3709ba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# DOC2VEC CLUSTERING (KMeans, SOM, HDBSCAN)\n",
    "# ======================================================\n",
    "\n",
    "# Select Doc2Vec vectors\n",
    "X_doc2vec = encoders[\"DOC2VEC\"]\n",
    "print(\"Using Doc2Vec embedding:\", X_doc2vec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14054535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans sweep\n",
    "best_km_doc, km_results_doc = cu.find_num_clusters(\n",
    "    X_doc2vec,\n",
    "    K_RANGE,\n",
    "    cluster_algo=\"kmeans\"\n",
    ")\n",
    "\n",
    "cu.plot_silhouette_trend(\n",
    "    km_results_doc,\n",
    "    \"KMeans Silhouette Trend (Doc2Vec)\"\n",
    ")\n",
    "\n",
    "chosen_k_doc = best_km_doc[\"k\"]\n",
    "\n",
    "labels_kmeans_doc2vec, model_kmeans_doc2vec = cu.cluster_traces(\n",
    "    X_doc2vec,\n",
    "    num_clusters=chosen_k_doc,\n",
    "    cluster_algo=\"kmeans\"\n",
    ")\n",
    "\n",
    "cu.mine_from_clusters(\n",
    "    labels_kmeans_doc2vec,\n",
    "    num_clusters=chosen_k_doc,\n",
    "    sequences_df=filtered.reset_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca67e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOM sweep\n",
    "best_som_doc, som_results_doc = cu.find_num_clusters(\n",
    "    X_doc2vec,\n",
    "    K_RANGE,\n",
    "    cluster_algo=\"som\"\n",
    ")\n",
    "\n",
    "cu.plot_silhouette_trend(\n",
    "    som_results_doc,\n",
    "    \"SOM Silhouette Trend (Doc2Vec)\"\n",
    ")\n",
    "\n",
    "labels_som_doc2vec, _ = cu.cluster_traces(\n",
    "    X_doc2vec,\n",
    "    num_clusters=best_som_doc[\"k\"],\n",
    "    cluster_algo=\"som\"\n",
    ")\n",
    "\n",
    "cu.mine_from_clusters(\n",
    "    labels_som_doc2vec,\n",
    "    num_clusters=best_som_doc[\"k\"],\n",
    "    sequences_df=filtered.reset_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd5d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_hdbscan_doc2vec, hdb_model_doc2vec = cu.cluster_traces(\n",
    "    X_doc2vec,\n",
    "    cluster_algo=\"hdbscan\"\n",
    ")\n",
    "\n",
    "cu.mine_from_clusters(\n",
    "    labels_hdbscan_doc2vec,\n",
    "    sequences_df=filtered.reset_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b690fd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered[\"cluster_kmeans_doc2vec\"]  = labels_kmeans_doc2vec\n",
    "filtered[\"cluster_som_doc2vec\"]     = labels_som_doc2vec\n",
    "filtered[\"cluster_hdbscan_doc2vec\"] = labels_hdbscan_doc2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1204f45",
   "metadata": {},
   "source": [
    "# Process Discovery - Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9425eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# Global Process Discovery Pipeline (Annotated Version)\n",
    "# =============================================================\n",
    "# This script performs *global* process discovery using PM4Py.\n",
    "# It discovers a single process model for the entire log (no clustering),\n",
    "# evaluates conformance (fitness + precision), visualizes the resulting\n",
    "# models (Petri net / BPMN / Heuristics Net), and computes variability.\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 0) Imports\n",
    "# =============================================================\n",
    "# PM4Py evaluation\n",
    "from pm4py.algo.evaluation.precision import algorithm as precision_evaluator\n",
    "from pm4py.algo.evaluation.replay_fitness.algorithm import Variants as FitnessVariants\n",
    "from pm4py.algo.evaluation.precision.algorithm import Variants as PrecisionVariants\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Log conversion utilities\n",
    "from pm4py.objects.log.util import dataframe_utils\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "\n",
    "# Fitness (token-based replay) and precision\n",
    "from pm4py.algo.evaluation.replay_fitness.variants import token_replay as fitness_evaluator\n",
    "from pm4py.algo.evaluation.precision import algorithm as precision_evaluator\n",
    "from pm4py.objects.log.util import dataframe_utils\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.algo.evaluation.replay_fitness.variants import token_replay as fitness_evaluator\n",
    "from pm4py.algo.evaluation.precision import algorithm as precision_evaluator\n",
    "\n",
    "from pm4py.visualization.heuristics_net import visualizer as hn_visualizer\n",
    "\n",
    "# =============================================================\n",
    "# 1) Configuration\n",
    "# =============================================================\n",
    "MINER_TYPE = \"inductive\"  # options: \"inductive\", \"heuristics\", \"alpha\"\n",
    "RANDOM_STATE = 42         # deterministic sampling\n",
    "EVAL_SAMPLE_SIZE = 2000   # conformance evaluation log sample size\n",
    "\n",
    "# Conformance settings\n",
    "# checks *fit* (does the model reproduce behavior?)\n",
    "FITNESS_VARIANT = FitnessVariants.TOKEN_BASED\n",
    "# checks *specificity* (does it avoid overgeneralizing?)\n",
    "PRECISION_VARIANT = PrecisionVariants.ETCONFORMANCE_TOKEN\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0e526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def discover_model_for_miner(log):\n",
    "    \"\"\"\n",
    "    Wrapper to call the correct PM4Py discovery algorithm based on MINER_TYPE.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    (net, im, fm, heu_net, process_tree)\n",
    "    - heuristics miner returns heu_net instead\n",
    "    - inductive miner returns process tree\n",
    "    \"\"\"\n",
    "    if MINER_TYPE == \"inductive\":\n",
    "        process_tree = inductive_miner.apply(log)\n",
    "        net, im, fm = pt_converter.apply(process_tree)\n",
    "        return net, im, fm, None, process_tree     # heuristics net = None\n",
    "\n",
    "    elif MINER_TYPE == \"alpha\":\n",
    "        net, im, fm = alpha_miner.apply(log)\n",
    "        return net, im, fm, None, None     # heuristics net = None\n",
    "\n",
    "    elif MINER_TYPE == \"heuristics\":\n",
    "        heu_net = heuristics_miner.apply_heu(log)\n",
    "        net, im, fm = hn_converter.apply(heu_net)\n",
    "        return net, im, fm, heu_net, None\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"‚ùå Unknown MINER_TYPE: {MINER_TYPE}\")\n",
    "\n",
    "# =============================================================\n",
    "# 3) Sampling Utility\n",
    "# =============================================================\n",
    "def maybe_sample_log(event_log, max_traces):\n",
    "    \"\"\"Sample a subset of traces from a log if it exceeds max_traces.\n",
    "\n",
    "    Args:\n",
    "        event_log (EventLog): PM4Py event log object.\n",
    "        max_traces (int): Maximum number of traces to keep.\n",
    "\n",
    "    Returns:\n",
    "        EventLog: A sampled or original log.\n",
    "    \"\"\"\n",
    "    if (max_traces is None) or (len(event_log) <= max_traces):\n",
    "        return event_log\n",
    "    idx = np.random.RandomState(RANDOM_STATE).choice(len(event_log), size=max_traces, replace=False)\n",
    "    return EventLog([event_log[i] for i in sorted(idx)])\n",
    "\n",
    "# =============================================================\n",
    "# 4) Variability Measure\n",
    "# =============================================================\n",
    "# Variant variability reflects behavioral diversity.\n",
    "def compute_variability_ratio(log):\n",
    "    \"\"\"\n",
    "    Compute variability ratio = (# unique variants) / (# total traces).\n",
    "    Lower values indicate more homogeneous clusters.\n",
    "\n",
    "    Args:\n",
    "        log (EventLog): PM4Py EventLog for the cluster.\n",
    "\n",
    "    Returns:\n",
    "        float: variability ratio\n",
    "    \"\"\"\n",
    "    if len(log) == 0:\n",
    "        return 0.0\n",
    "    variant_set = set()\n",
    "    for trace in log:\n",
    "        seq = tuple(e[\"concept:name\"] for e in trace)\n",
    "        variant_set.add(seq)\n",
    "    return len(variant_set) / len(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac038dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section MUST be ran before Process discovery\n",
    "from pm4py.algo.evaluation.replay_fitness import algorithm as fitness_evaluator\n",
    "from pm4py.algo.evaluation.precision import algorithm as precision_evaluator\n",
    "\n",
    "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
    "from pm4py.objects.conversion.process_tree import converter as pt_converter\n",
    "\n",
    "from pm4py.algo.discovery.heuristics import algorithm as heuristics_miner\n",
    "from pm4py.objects.conversion.heuristics_net import converter as hn_converter\n",
    "\n",
    "from pm4py.algo.evaluation.replay_fitness.algorithm import Variants as FitnessVariants\n",
    "from pm4py.algo.evaluation.precision.algorithm import Variants as PrecisionVariants\n",
    "\n",
    "from pm4py.visualization.petri_net import visualizer as pn_visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d5af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.visualization.petri_net import visualizer as pn_visualizer\n",
    "from pm4py.objects.conversion.process_tree import converter as process_tree_converter\n",
    "from pm4py.visualization.bpmn import visualizer as bpmn_visualizer\n",
    "\n",
    "# =============================================================\n",
    "# 5) Global Process Discovery + Evaluation\n",
    "# =============================================================\n",
    "def evaluate_global_model(df, metrics_df=None):\n",
    "    \"\"\"\n",
    "    Perform discovery ‚Üí visualization ‚Üí conformance evaluation for a *global* model.\n",
    "\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): Raw event log with columns:\n",
    "    - case:concept:name\n",
    "    - concept:name\n",
    "    - time:timestamp\n",
    "\n",
    "\n",
    "    metrics_df (pd.DataFrame or None):\n",
    "    ‚Äì If provided, append new results.\n",
    "    ‚Äì If None, create the table.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Updated metrics table.\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # Convert pandas ‚Üí PM4Py EventLog\n",
    "    # ----------------------------------------------\n",
    "    params = {log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: \"case:concept:name\"}\n",
    "    global_log = log_converter.apply(df, variant=log_converter.Variants.TO_EVENT_LOG, parameters=params)\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # Model discovery\n",
    "    # ----------------------------------------------\n",
    "    net, im, fm, heu_net, process_tree = discover_model_for_miner(global_log)\n",
    "    \n",
    "    # ----------------------------------------------\n",
    "    # Visualization (Petri net, BPMN, or heuristics net)\n",
    "    # ----------------------------------------------\n",
    "   # --- Visualization settings ---\n",
    "    if MINER_TYPE == \"inductive\":\n",
    "        VISUALIZATION_THRESHOLD = None  # set to None = no size limit\n",
    "\n",
    "        #from pm4py.visualization.petri_net import visualizer as pn_visualizer\n",
    "\n",
    "        print(\"PN size:\", len(net.places), len(net.transitions))\n",
    "\n",
    "        if VISUALIZATION_THRESHOLD is None or len(net.transitions) < VISUALIZATION_THRESHOLD:\n",
    "            #print(\"Rendering PN...\")\n",
    "            #gviz = pn_visualizer.apply(net, im, fm)\n",
    "            #pn_visualizer.view(gviz)\n",
    "            print(\"Rendering BPMN...\")\n",
    "            bpmn_graph = process_tree_converter.apply(\n",
    "            process_tree,\n",
    "            variant=process_tree_converter.Variants.TO_BPMN)\n",
    "            \n",
    "            gviz = bpmn_visualizer.apply(bpmn_graph)\n",
    "            bpmn_visualizer.view(gviz)\n",
    "        else:\n",
    "            print(f\"PN too large ({len(net.transitions)} transitions) ‚Äî skipping visualization\")\n",
    "            \n",
    "    elif MINER_TYPE == \"heuristics\":\n",
    "        # heuristics nets need their own visualizer\n",
    "        \n",
    "        print(\"Rendering Heuristics Net...\")\n",
    "        gviz = hn_visualizer.apply(heu_net)\n",
    "        hn_visualizer.view(gviz)\n",
    "    # --- END ADDED BLOCK ---\n",
    "    \n",
    "    # ----------------------------------------------\n",
    "    # Conformance checking\n",
    "    # ----------------------------------------------\n",
    "    # maybe sample\n",
    "    eval_log = maybe_sample_log(global_log, EVAL_SAMPLE_SIZE)\n",
    "\n",
    "    # conformance eval\n",
    "    fit_res  = fitness_evaluator.apply(eval_log, net, im, fm, variant=FITNESS_VARIANT)\n",
    "    fitness  = fit_res.get(\"average_trace_fitness\", fit_res.get(\"perc_fit_traces\", np.nan))\n",
    "    precision = precision_evaluator.apply(eval_log, net, im, fm, variant=PRECISION_VARIANT)\n",
    "    fscore    = 2 * (precision * fitness) / (precision + fitness) if (precision + fitness) > 0 else 0.0\n",
    "\n",
    "    # variability\n",
    "    global_variability = compute_variability_ratio(global_log)\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # Build result row\n",
    "    # ----------------------------------------------\n",
    "    row_dict = {\n",
    "        \"Method\": \"Global\",\n",
    "        \"Cluster\": \"Global\",\n",
    "        \"NumTraces\": len(global_log),\n",
    "        \"Precision\": float(precision),\n",
    "        \"Fitness\": float(fitness),\n",
    "        \"FScore\": float(fscore),\n",
    "        \"VariabilityRatio\": float(global_variability)\n",
    "    }\n",
    "\n",
    "    # Initialize or append\n",
    "    if (metrics_df is None) or (len(metrics_df) == 0):\n",
    "        metrics_df = pd.DataFrame([row_dict])\n",
    "    else:\n",
    "        # subsequent calls\n",
    "        metrics_df = pd.concat([metrics_df, pd.DataFrame([row_dict])], ignore_index=True)\n",
    "\n",
    "    print(\n",
    "        f\"üåê Global Model ({MINER_TYPE}) ‚Üí \"\n",
    "        f\"Precision: {precision:.3f}, Fitness: {fitness:.3f}, \"\n",
    "        f\"F-score: {fscore:.3f}, Variability Ratio: {global_variability:.3f}\"\n",
    "    )\n",
    "\n",
    "    return metrics_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2baff58",
   "metadata": {},
   "source": [
    "## When MINER_TYPE = \"heuristics\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525b6ccd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "metrics_df = evaluate_global_model(df)\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df54abd7",
   "metadata": {},
   "source": [
    "## When MINER_TYPE = \"inductive\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc4bdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics_df = evaluate_global_model(df)\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14962e89",
   "metadata": {},
   "source": [
    "# Process Discovery - Per Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f1c232",
   "metadata": {},
   "source": [
    "## Inductive Miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# Per‚ÄëCluster Process Discovery & Evaluation (Annotated Version)\n",
    "# =============================================================\n",
    "# This script mirrors the global process discovery workflow but applied\n",
    "# separately to each cluster. For every cluster, we:\n",
    "# 1. Extract all traces belonging to that cluster\n",
    "# 2. Convert them to an EventLog\n",
    "# 3. Discover a process model (IMf version of Inductive Miner)\n",
    "# 4. Optionally visualize the model (BPMN)\n",
    "# 5. Compute conformance (precision, fitness, F-score)\n",
    "# 6. Compute variability ratio\n",
    "# 7. Append results to a cluster‚Äëlevel metrics table\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 0) Discovery Function ‚Äî IMf Variant\n",
    "# =============================================================\n",
    "# IMf = Inductive Miner (infrequent) ‚Äî a *more flexible* configuration.\n",
    "# It captures more behavioral detail (higher fitness) at the cost of more\n",
    "# complex / less generalizable models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcadb379",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---BALANCED-----\n",
    "# Keeps most traces\n",
    "# Removes some infrequent paths\n",
    "# Produces a reasonably interpretable model\n",
    "\n",
    "# --- NEW: IMf variant specifically for per-cluster discovery ---\n",
    "def discover_model_for_miner_imf(log):\n",
    "    \"\"\"\n",
    "    IMf version ‚Äî matches old pipeline behavior.\n",
    "    \"\"\"\n",
    "    tree = inductive_miner.apply(\n",
    "        log,\n",
    "        variant=inductive_miner.Variants.IMf,\n",
    "        parameters={\"noise_threshold\": 0.2}\n",
    "    )\n",
    "    net, im, fm = pt_converter.apply(tree)\n",
    "    return net, im, fm, tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485b554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---STRICT(simpler models, less fitness)-----\n",
    "# Filters weak paths aggressively\n",
    "# Great for large noisy logs\n",
    "# Model may underfit (i.e. lose rare but valid behavior)\n",
    "#def discover_model_for_miner_imf(log):\n",
    "    \"\"\"\n",
    "    IMf version ‚Äî matches old pipeline behavior.\n",
    "    \"\"\"\n",
    "    #tree = inductive_miner.apply(\n",
    "        #log,\n",
    "        #variant=inductive_miner.Variants.IMf,\n",
    "        #parameters={\n",
    "            #\"noise_threshold\": 0.4,\n",
    "            #\"min_dfg_occurrences\": 2\n",
    "        #}\n",
    "    #)\n",
    "    #net, im, fm = pt_converter.apply(tree)\n",
    "    #return net, im, fm, tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453fd7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---FLEXIBLE(complex model, higher fitness)-----\n",
    "# Almost no pruning\n",
    "# captures most behavior\n",
    "# Model may overfit(i.e. messy and less generalizable)\n",
    "#def discover_model_for_miner_imf(log):\n",
    "    \"\"\"\n",
    "    IMf version ‚Äî matches old pipeline behavior.\n",
    "    \"\"\"\n",
    "    #tree = inductive_miner.apply(\n",
    "        #log,\n",
    "        #variant=inductive_miner.Variants.IMf,\n",
    "        #parameters={\n",
    "            #\"noise_threshold\": 0.1,\n",
    "            #\"min_dfg_occurrences\": 1\n",
    "        #}\n",
    "    #)\n",
    "    #net, im, fm = pt_converter.apply(tree)\n",
    "    #return net, im, fm, tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0279a135",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================\n",
    "# 1) Per‚ÄëCluster Discovery & Evaluation\n",
    "# =============================================================\n",
    "\n",
    "def discover_and_evaluate_per_cluster(\n",
    "    df: pd.DataFrame,\n",
    "    filtered: pd.DataFrame,\n",
    "    cluster_col: str,\n",
    "    method_name: str = None,\n",
    "    cluster_metrics_df: pd.DataFrame = None,   # ‚Üê renamed\n",
    "    skip_noise: bool = True,\n",
    "    noise_label: int = -1,\n",
    "    visualize: bool = False,\n",
    "    outdir: str = \"cluster_models\",\n",
    "    sample_size: int = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Discover and evaluate process models *independently for each cluster*.\n",
    "\n",
    "\n",
    "    Args:\n",
    "    df: Original full event log (pandas DataFrame).\n",
    "    filtered: DataFrame containing case IDs + cluster assignments.\n",
    "    cluster_col: Column name inside `filtered` identifying cluster labels.\n",
    "    method_name: Name to insert in the metrics output (defaults to cluster_col).\n",
    "    cluster_metrics_df: Existing table to append results to.\n",
    "    skip_noise: If True, skip the noise cluster (e.g., HDBSCAN label -1).\n",
    "    noise_label: Integer label representing noise.\n",
    "    visualize: If True, render BPMN for each cluster.\n",
    "    outdir: Directory for saving models (not used here but reserved).\n",
    "    sample_size: Max traces to evaluate (None ‚Üí default = EVAL_SAMPLE_SIZE).\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    Updated cluster_metrics_df containing one row per cluster.\n",
    "    \"\"\"\n",
    "    method_name = method_name or cluster_col\n",
    "    #sample_size = sample_size if sample_size is not None else EVAL_SAMPLE_SIZE\n",
    "    \n",
    "    # ----------------------------------------------------------\n",
    "    # Validate index structure (cases must be index or a column)\n",
    "    # ----------------------------------------------------------\n",
    "    if filtered.index.name != \"case:concept:name\":\n",
    "        if \"case:concept:name\" in filtered.columns:\n",
    "            filtered = filtered.set_index(\"case:concept:name\", drop=True)\n",
    "        else:\n",
    "            raise ValueError(\"`filtered` must have case ids on the index or a 'case:concept:name' column.\")\n",
    "\n",
    "    if cluster_col not in filtered.columns:\n",
    "        raise ValueError(f\"`{cluster_col}` not found in filtered columns: {list(filtered.columns)}\")\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Iterate over clusters\n",
    "    # ----------------------------------------------------------\n",
    "    for c, case_ids in filtered.groupby(cluster_col).groups.items():\n",
    "        if skip_noise and c == noise_label:\n",
    "            continue\n",
    "\n",
    "        cluster_df = df[df[\"case:concept:name\"].isin(case_ids)].copy()\n",
    "        if cluster_df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Extract cluster-specific subset of df\n",
    "        cluster_df = dataframe_utils.convert_timestamp_columns_in_df(cluster_df)\n",
    "        \n",
    "        # Standard timestamp normalization for PM4Py\n",
    "        cluster_df = cluster_df.sort_values(\n",
    "            [\"case:concept:name\",\"time:timestamp\"],\n",
    "            ignore_index=True\n",
    "        )\n",
    "        # Mapping for PM4Py conversion\n",
    "        params = {\n",
    "            \"case_id\":       \"case:concept:name\",\n",
    "            \"activity_key\":  \"concept:name\",\n",
    "            \"timestamp_key\": \"time:timestamp\",\n",
    "        }\n",
    "        \n",
    "        \n",
    "\n",
    "        log = log_converter.apply(cluster_df, variant=log_converter.Variants.TO_EVENT_LOG, parameters=params)\n",
    "\n",
    "        # ------------------------------------------------------\n",
    "        # Discover model (IMf variant for detailed structure)\n",
    "        # ------------------------------------------------------\n",
    "        net, im, fm, tree = discover_model_for_miner_imf(log)\n",
    "\n",
    "        eval_log = log\n",
    "        \n",
    "        # ------------------------------------------------------\n",
    "        # Conformance metrics\n",
    "        # ------------------------------------------------------\n",
    "        fit_res  = fitness_evaluator.apply(eval_log, net, im, fm, variant=FITNESS_VARIANT)\n",
    "        fitness  = fit_res.get(\"log_fitness\", None)\n",
    "\n",
    "        precision = precision_evaluator.apply(eval_log, net, im, fm, variant=PRECISION_VARIANT)\n",
    "        \n",
    "        if precision is not None and fitness is not None and (precision+fitness) > 0:\n",
    "            fscore    = (2 * precision * fitness / (precision + fitness))  \n",
    "        else: \n",
    "            fscore = 0.0\n",
    "\n",
    "        variability = compute_variability_ratio(log)\n",
    "        \n",
    "        # ------------------------------------------------------\n",
    "        # Append metrics row\n",
    "        # ------------------------------------------------------\n",
    "\n",
    "        row_dict = {\n",
    "            \n",
    "            \"Method\":           method_name,\n",
    "            \"Cluster\":          f\"Cluster {c}\",\n",
    "            \"NumTraces\":        len(log),\n",
    "            \"Precision\":        float(precision),\n",
    "            \"Fitness\":          float(fitness),\n",
    "            \"FScore\":           float(fscore),\n",
    "            \"VariabilityRatio\": float(variability),\n",
    "            \"Miner\":            \"inductive\"\n",
    "        }\n",
    "\n",
    "        # ‚Üê saving to the cluster-level dataframe\n",
    "        if (cluster_metrics_df is None) or (len(cluster_metrics_df) == 0):\n",
    "            cluster_metrics_df = pd.DataFrame([row_dict])\n",
    "        else:\n",
    "            cluster_metrics_df = pd.concat(\n",
    "                [cluster_metrics_df, pd.DataFrame([row_dict])],\n",
    "                ignore_index=True\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            f\"‚úÖ {cluster_col} / Cluster {c} (IMf) ‚Üí \"\n",
    "            f\"Prec {precision:.3f} | Fit {fitness:.3f} | F1 {fscore:.3f} | Var {variability:.3f} | \"\n",
    "            f\"Traces {len(log)}\"\n",
    "        )\n",
    "        \n",
    "        # ------------------------------------------------------\n",
    "        # Optional visualization\n",
    "        # ------------------------------------------------------\n",
    "\n",
    "        if visualize:\n",
    "            #print(f\"Rendering PN for cluster {c}...\")\n",
    "            #gviz = pn_visualizer.apply(net, im, fm)\n",
    "            #pn_visualizer.view(gviz)\n",
    "            print(f\"Rendering BPMN for cluster {c}...\")\n",
    "            bpmn_graph = process_tree_converter.apply(\n",
    "                            tree,\n",
    "                            variant=process_tree_converter.Variants.TO_BPMN\n",
    "            )\n",
    "            gviz = bpmn_visualizer.apply(bpmn_graph)\n",
    "            bpmn_visualizer.view(gviz)\n",
    "\n",
    "    return cluster_metrics_df    # important for downstream analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30301c5a",
   "metadata": {},
   "source": [
    "### TFIDF Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd95317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kmeans + TFIDF  (FIRST)\n",
    "cluster_metrics_df = discover_and_evaluate_per_cluster(\n",
    "    df=df,\n",
    "    filtered=filtered,\n",
    "    cluster_col=\"cluster_kmeans_tfidf_svd\",\n",
    "    method_name=\"KMeans-TFIDF\",\n",
    "    cluster_metrics_df=None,          # ‚Üê When set to none, it resets the dataframe cluster_metrics_df\n",
    "    skip_noise=True,\n",
    "    visualize=True\n",
    ")\n",
    "\n",
    "# SOM + TFIDF\n",
    "cluster_metrics_df = discover_and_evaluate_per_cluster(\n",
    "    df=df,\n",
    "    filtered=filtered,\n",
    "    cluster_col=\"cluster_som_tfidf_svd\",\n",
    "    method_name=\"SOM-TFIDF\",\n",
    "    cluster_metrics_df=cluster_metrics_df,  # << append, not None\n",
    "    skip_noise=True,\n",
    "    visualize=True\n",
    ")\n",
    "\n",
    "# HDBSCAN + TFIDF\n",
    "cluster_metrics_df = discover_and_evaluate_per_cluster(\n",
    "    df=df,\n",
    "    filtered=filtered,\n",
    "    cluster_col=\"cluster_hdbscan_tfidf_svd\",\n",
    "    method_name=\"HDBSCAN-TFIDF\",\n",
    "    cluster_metrics_df=cluster_metrics_df,  # << append, not None\n",
    "    skip_noise=True,\n",
    "    noise_label=-1,\n",
    "    visualize=True\n",
    ")\n",
    "\n",
    "display(cluster_metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898870f8",
   "metadata": {},
   "source": [
    "### Doc2Vec Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e865d27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KMeans + Doc2Vec\n",
    "cluster_metrics_df = discover_and_evaluate_per_cluster(\n",
    "    df=df,\n",
    "    filtered=filtered,\n",
    "    cluster_col=\"cluster_kmeans_doc2vec\",\n",
    "    method_name=\"KMeans-Doc2Vec\",\n",
    "    cluster_metrics_df=cluster_metrics_df,     # first call in this block\n",
    "    skip_noise=True,\n",
    "    visualize=True\n",
    ")\n",
    "\n",
    "# SOM + Doc2Vec\n",
    "cluster_metrics_df = discover_and_evaluate_per_cluster(\n",
    "    df=df,\n",
    "    filtered=filtered,\n",
    "    cluster_col=\"cluster_som_doc2vec\",\n",
    "    method_name=\"SOM-Doc2Vec\",\n",
    "    cluster_metrics_df=cluster_metrics_df,   # append to same df\n",
    "    skip_noise=True,\n",
    "    visualize=True\n",
    ")\n",
    "\n",
    "# HDBSCAN + Doc2Vec\n",
    "cluster_metrics_df = discover_and_evaluate_per_cluster(\n",
    "    df=df,\n",
    "    filtered=filtered,\n",
    "    cluster_col=\"cluster_hdbscan_doc2vec\",\n",
    "    method_name=\"HDBSCAN-Doc2Vec\",\n",
    "    cluster_metrics_df=cluster_metrics_df,   # append again\n",
    "    skip_noise=True,\n",
    "    noise_label=-1,\n",
    "    visualize=True\n",
    ")\n",
    "\n",
    "\n",
    "display(cluster_metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1c5358",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_metrics_df.to_csv(\"cluster_metrics_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88009c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all methods / encoders\n",
    "all_models = cluster_metrics_df\n",
    "\n",
    "# Weighted by number of traces\n",
    "weights = all_models[\"NumTraces\"]\n",
    "\n",
    "weighted_avg_all = (\n",
    "    all_models[[\"Precision\", \"Fitness\", \"FScore\"]]\n",
    "    .multiply(weights, axis=0)\n",
    "    .sum() / weights.sum()\n",
    ")\n",
    "\n",
    "weighted_avg_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bc5b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ALL methods (no filtering)\n",
    "all_methods = cluster_metrics_df\n",
    "\n",
    "# Weighted averages per method (all encoders + models included)\n",
    "method_weighted_all = (\n",
    "    all_methods\n",
    "    .groupby(\"Method\")\n",
    "    .apply(lambda g: (\n",
    "        (g[[\"Precision\",\"Fitness\",\"FScore\"]] * g[\"NumTraces\"].values[:, None]).sum()\n",
    "        / g[\"NumTraces\"].sum()\n",
    "    ))\n",
    ")\n",
    "\n",
    "method_weighted_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1ce2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_counts_per_method = (\n",
    "    cluster_metrics_df\n",
    "    .groupby(\"Method\")[\"NumTraces\"]\n",
    "    .sum()\n",
    "    .rename(\"TotalTraces\")\n",
    ")\n",
    "\n",
    "trace_counts_per_method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839e075f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TextVec2 Env",
   "language": "python",
   "name": "textvec2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
