{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f19a22",
   "metadata": {},
   "source": [
    "# Path Config & Global Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a70a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# -----------------------\n",
    "# üîß Project Path Config\n",
    "# -----------------------\n",
    "# Set project base path dynamically using the folder where the notebook is opened.\n",
    "# This makes the notebook fully portable across different machines without hard-coded directories.\n",
    "\n",
    "# Use the folder where the notebook is opened\n",
    "# This means the CSV is expected to be in the same folder as the notebook, and Python finds it \n",
    "#  relative to the notebook location\n",
    "BASE_PATH = Path.cwd()   # used it everywhere a file path is needed\n",
    "print(\"Working in:\", BASE_PATH)\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"  # must be set before importing numpy/sklearn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"'force_all_finite' was renamed to 'ensure_all_finite'\",\n",
    "    category=FutureWarning\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd5dd43",
   "metadata": {},
   "source": [
    "# Load Data and Create Trace Strings for Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a078e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# Vectorization & Clustering Pipeline (Annotated Version)\n",
    "# ======================================================\n",
    "\n",
    "\n",
    "# This script loads event-log data, constructs textual variants of cases,\n",
    "# encodes them using TF-IDF and Doc2Vec, reduces dimensionality when needed,\n",
    "# and runs several clustering algorithms (KMeans, SOM, HDBSCAN). Metrics and\n",
    "# cluster-level summaries are also computed.\n",
    "\n",
    "# ======================================================\n",
    "# 0) Imports & Global Config\n",
    "# ======================================================\n",
    "# Standard library\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Numerical / Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# ML / NLP\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from minisom import MiniSom\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# ======================================================\n",
    "# Global Random Seed (for reproducibility across modules)\n",
    "# ======================================================\n",
    "GLOBAL_SEED = 42\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "random.seed(GLOBAL_SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(GLOBAL_SEED)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 1) Load Data\n",
    "# ======================================================\n",
    "# The script expects a CSV containing an event log with:\n",
    "# - case:concept:name (case IDs)\n",
    "# - time:timestamp (event timestamp)\n",
    "# - concept:name (activity name)\n",
    "# Sampled subset is used for feasibility.\n",
    "log_path = BASE_PATH / \"df_sampled_100_cases.csv\"\n",
    "\n",
    "df = pd.read_csv(log_path)\n",
    "\n",
    "df[\"time:timestamp\"] = pd.to_datetime(df[\"time:timestamp\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"time:timestamp\"])\n",
    "df = df.sort_values([\"case:concept:name\", \"time:timestamp\"]).copy()\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 2) Variant Filtering\n",
    "# ======================================================\n",
    "# Each case is turned into a trace string: \"A B C D\" representing activity sequence.\n",
    "# We count identical variants and group rare ones into a single \"__OTHER__\" bucket.\n",
    "print(\"\\nüîç Variant frequency analysis...\")\n",
    "\n",
    "# Build the trace strings per case\n",
    "seqs = (\n",
    "    df.groupby(\"case:concept:name\")[\"concept:name\"]\n",
    "    .apply(lambda s: \" \".join(s.astype(str).tolist()))\n",
    "    .rename(\"trace_str\")\n",
    "    .to_frame()\n",
    ")\n",
    "\n",
    "# Count frequency of each unique variant\n",
    "variant_counts = (\n",
    "    seqs[\"trace_str\"]\n",
    "    .value_counts()\n",
    "    .rename_axis(\"variant\")\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "variant_counts[\"pct\"] = variant_counts[\"count\"] / variant_counts[\"count\"].sum()\n",
    "\n",
    "# Keep variants with at least this frequency; others lumped\n",
    "min_variant_freq = 2\n",
    "common_variants = set(\n",
    "    variant_counts.loc[variant_counts[\"count\"] >= min_variant_freq, \"variant\"]\n",
    ")\n",
    "\n",
    "# Add a \"variant\" column assigning rare cases to \"__OTHER__\"\n",
    "filtered = seqs.copy()\n",
    "filtered[\"variant\"] = filtered[\"trace_str\"].where(\n",
    "    filtered[\"trace_str\"].isin(common_variants), \"__OTHER__\"\n",
    ")\n",
    "\n",
    "# Summary table for transparency\n",
    "filter_stats = pd.DataFrame({\n",
    "    \"total_cases\": [len(seqs)],\n",
    "    \"unique_variants_total\": [variant_counts.shape[0]],\n",
    "    \"min_variant_freq\": [min_variant_freq],\n",
    "    \"unique_variants_kept\": [len(common_variants) + (1 if \"__OTHER__\" in filtered[\"variant\"].unique() else 0)],\n",
    "    \"cases_in_other\": [(filtered[\"variant\"] == \"__OTHER__\").sum()]\n",
    "})\n",
    "\n",
    "print(\"\\nüìä Variant filtering summary:\")\n",
    "display(filter_stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8cf895",
   "metadata": {},
   "source": [
    "# Vectorization and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138a26e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 3) Vectorization Function\n",
    "# ======================================================\n",
    "def vectorize(sequences, method=\"both\"):\n",
    "    \"\"\"\n",
    "    Vectorize trace sequences into numerical embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sequences : pd.Series\n",
    "        One trace string per case, e.g. 'A B C D'.\n",
    "    method : {'both', 'tfidf', 'doc2vec'}\n",
    "        Which embeddings to compute.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    encoders : dict\n",
    "        Mapping name -> 2D numpy array of embeddings.\n",
    "        Keys will include 'TFIDF_SVD' and/or 'DOC2VEC'.\n",
    "    artifacts : dict\n",
    "        Any fitted objects (tfidf, svd, doc2vec_model) for reuse.\n",
    "    \"\"\"\n",
    "    encoders = {}\n",
    "    artifacts = {}\n",
    "\n",
    "    # --- TF-IDF + SVD ---\n",
    "    if method in (\"both\", \"tfidf\"):\n",
    "        tfidf = TfidfVectorizer(\n",
    "            token_pattern=r\"[^ ]+\",\n",
    "            lowercase=False,\n",
    "            ngram_range=(1, 3)\n",
    "        )\n",
    "        X_tfidf = tfidf.fit_transform(sequences)\n",
    "\n",
    "        svd_components = max(min(10, X_tfidf.shape[1] - 1), 2)\n",
    "        svd = TruncatedSVD(n_components=svd_components, random_state=GLOBAL_SEED)\n",
    "        X_tfidf_svd = svd.fit_transform(X_tfidf)\n",
    "\n",
    "        X_tfidf_std = StandardScaler().fit_transform(X_tfidf_svd)\n",
    "\n",
    "        encoders[\"TFIDF_SVD\"] = X_tfidf_std\n",
    "        artifacts[\"tfidf\"] = tfidf\n",
    "        artifacts[\"svd\"] = svd\n",
    "\n",
    "    # --- Doc2Vec ---\n",
    "    if method in (\"both\", \"doc2vec\"):\n",
    "        tagged_docs = [\n",
    "            TaggedDocument(words=trace.split(), tags=[str(i)])\n",
    "            for i, trace in enumerate(sequences)\n",
    "        ]\n",
    "        doc2vec_model = Doc2Vec(\n",
    "            vector_size=100,\n",
    "            min_count=1,\n",
    "            epochs=40,\n",
    "            seed=GLOBAL_SEED,\n",
    "            workers=1\n",
    "        )\n",
    "        doc2vec_model.build_vocab(tagged_docs)\n",
    "        doc2vec_model.train(\n",
    "            tagged_docs,\n",
    "            total_examples=doc2vec_model.corpus_count,\n",
    "            epochs=doc2vec_model.epochs\n",
    "        )\n",
    "        X_doc2vec = np.array([\n",
    "            doc2vec_model.infer_vector(trace.split(), epochs=40, alpha=0.025)\n",
    "            for trace in sequences\n",
    "        ])\n",
    "        X_doc2vec_std = StandardScaler().fit_transform(X_doc2vec)\n",
    "\n",
    "        encoders[\"DOC2VEC\"] = X_doc2vec_std\n",
    "        artifacts[\"doc2vec_model\"] = doc2vec_model\n",
    "\n",
    "    print(\"\\n‚úÖ Embedding shapes:\")\n",
    "    for name, X in encoders.items():\n",
    "        print(f\"  - {name}: {X.shape}\")\n",
    "\n",
    "    return encoders, artifacts\n",
    "\n",
    "\n",
    "# Call the vectorization function on your trace sequences\n",
    "encoders, encoder_artifacts = vectorize(seqs[\"trace_str\"], method=\"both\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 4) Core Clustering Helpers (as before)\n",
    "# ======================================================\n",
    "def evaluate_clustering(X, labels):\n",
    "    \"\"\"Compute three internal clustering metrics.\n",
    "    If the clustering degenerates (one cluster or all unique labels), return NaN.\n",
    "    \"\"\"\n",
    "    unique = np.unique(labels)\n",
    "    if len(unique) < 2 or len(unique) == len(labels):\n",
    "        return {\"silhouette\": np.nan, \"ch\": np.nan, \"db\": np.nan}\n",
    "    return {\n",
    "        \"silhouette\": silhouette_score(X, labels),\n",
    "        \"ch\": calinski_harabasz_score(X, labels),\n",
    "        \"db\": davies_bouldin_score(X, labels)\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# KMeans sweep\n",
    "# ------------------------------------------------------\n",
    "def run_kmeans_sweep(X, k_values):\n",
    "    \"\"\"Try multiple k values and return both all results and the best one.\"\"\"\n",
    "    best = None\n",
    "    all_res = []\n",
    "\n",
    "    for k in k_values:\n",
    "        km = KMeans(n_clusters=k, n_init=20, random_state=GLOBAL_SEED)\n",
    "        labels = km.fit_predict(X)\n",
    "        metrics = evaluate_clustering(X, labels)\n",
    "\n",
    "        result = {\"k\": k, \"model\": km, \"labels\": labels, **metrics}\n",
    "        all_res.append(result)\n",
    "\n",
    "        if best is None or (\n",
    "            metrics[\"silhouette\"] > best[\"silhouette\"] or\n",
    "            (\n",
    "                np.isclose(metrics[\"silhouette\"], best[\"silhouette\"]) and\n",
    "                metrics[\"ch\"] > best[\"ch\"]\n",
    "            )\n",
    "        ):\n",
    "            best = result\n",
    "\n",
    "    return best, all_res\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# SOM without BMUs\n",
    "# ------------------------------------------------------\n",
    "def run_som_no_bmu(X, n_clusters, xdim=5, ydim=5,\n",
    "                   sigma=1.0, learning_rate=0.5, iters=5000):\n",
    "    \"\"\"Train a SOM and cluster neuron weights using KMeans.\n",
    "    Each data point is mapped to the label of its closest neuron.\n",
    "    \"\"\"\n",
    "    # SOM expects normalized input\n",
    "    X_scaled = MinMaxScaler().fit_transform(X)\n",
    "    \n",
    "    # Initialize SOM grid\n",
    "    som = MiniSom(\n",
    "        x=xdim, y=ydim,\n",
    "        input_len=X_scaled.shape[1],\n",
    "        sigma=sigma,\n",
    "        learning_rate=learning_rate,\n",
    "        random_seed=42\n",
    "    )\n",
    "\n",
    "    som.random_weights_init(X_scaled)\n",
    "    som.train_random(X_scaled, iters)\n",
    "    \n",
    "    # Flatten neuron weight vectors\n",
    "    weights = som.get_weights().reshape(-1, X_scaled.shape[1])\n",
    "    \n",
    "    # Cluster neurons\n",
    "    km = KMeans(n_clusters=n_clusters, n_init=20, random_state=GLOBAL_SEED)\n",
    "    neuron_labels = km.fit_predict(weights)\n",
    "    \n",
    "    # Map each data point to its nearest neuron\n",
    "    dists = np.linalg.norm(\n",
    "        X_scaled[:, None, :] - weights[None, :, :],\n",
    "        axis=2\n",
    "    )\n",
    "    closest = dists.argmin(axis=1)\n",
    "\n",
    "    return neuron_labels[closest]\n",
    "\n",
    "\n",
    "# Sweep over K for SOM\n",
    "# ------------------------------------------------------\n",
    "def run_som_sweep(X, k_values):\n",
    "    \"\"\"\n",
    "    Try different values of k (number of clusters) for a Self-Organizing Map (SOM).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        The embedding matrix for all traces (e.g., TF-IDF or Doc2Vec vectors).\n",
    "    k_values : iterable\n",
    "        A list or range of k values to evaluate, such as range(2, 16).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best : dict\n",
    "        The single best clustering result found across all k values.\n",
    "        Contains:\n",
    "            - 'k'          : the chosen number of clusters\n",
    "            - 'labels'     : cluster labels for each trace\n",
    "            - metrics...   : silhouette, CH, DB scores\n",
    "    all_res : list of dict\n",
    "        A list of the clustering results for *every* k tested.\n",
    "        Useful for plotting elbow/silhouette trends.\n",
    "    \"\"\"\n",
    "\n",
    "    best = None         # will store the best performing clustering configuration\n",
    "    all_res = []        # will store all results for later analysis\n",
    "\n",
    "    # Try each possible number of clusters\n",
    "    for k in k_values:\n",
    "\n",
    "        # Run SOM-based clustering for this k\n",
    "        labels = run_som_no_bmu(X, n_clusters=k)\n",
    "\n",
    "        # Compute internal cluster quality metrics\n",
    "        metrics = evaluate_clustering(X, labels)\n",
    "\n",
    "        # Package results for this k\n",
    "        result = {\"k\": k, \"labels\": labels, **metrics}\n",
    "        all_res.append(result)\n",
    "\n",
    "        # Keep track of the best result so far:\n",
    "        # Priority 1 ‚Üí max silhouette\n",
    "        # Priority 2 ‚Üí max Calinski-Harabasz score (if silhouette ties)\n",
    "        if best is None or (\n",
    "            metrics[\"silhouette\"] > best[\"silhouette\"] or\n",
    "            (\n",
    "                np.isclose(metrics[\"silhouette\"], best[\"silhouette\"]) and\n",
    "                metrics[\"ch\"] > best[\"ch\"]\n",
    "            )\n",
    "        ):\n",
    "            best = result\n",
    "\n",
    "    return best, all_res\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# HDBSCAN\n",
    "# ------------------------------------------------------\n",
    "def run_hdbscan(X, min_cluster_size=5):\n",
    "    \"\"\"Run density-based clustering. Cluster count is determined automatically.\"\"\"\n",
    "    hdb = HDBSCAN(min_cluster_size=min_cluster_size,\n",
    "                  min_samples=min_cluster_size,\n",
    "                  metric=\"euclidean\",\n",
    "                  cluster_selection_method=\"eom\")\n",
    "\n",
    "    labels = hdb.fit_predict(X)\n",
    "    metrics = evaluate_clustering(X, labels)\n",
    "\n",
    "    return {\n",
    "        \"model\": hdb,\n",
    "        \"labels\": labels,\n",
    "        \"k\": len(np.unique(labels[labels >= 0])),\n",
    "        **metrics\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Cluster summary\n",
    "# ------------------------------------------------------\n",
    "def cluster_summary(labels, seqs_df):\n",
    "    \"\"\"Compute summary statistics for each cluster: size, trace lengths,\n",
    "    entropy of variants, etc.\"\"\"\n",
    "    tmp = seqs_df.copy()\n",
    "    tmp[\"_cluster_tmp\"] = labels\n",
    "\n",
    "    rows = []\n",
    "    for c, g in tmp.groupby(\"_cluster_tmp\"):\n",
    "        n = len(g)\n",
    "        lengths = g[\"trace_str\"].str.split().map(len)\n",
    "        vc = g[\"variant\"].value_counts(normalize=True)\n",
    "        # Variant entropy approximates intra-cluster variability\n",
    "        entropy = -np.sum(vc * np.log2(vc + 1e-12))\n",
    "\n",
    "        rows.append({\n",
    "            \"cluster\": int(c),\n",
    "            \"size\": n,\n",
    "            \"pct\": round(100*n/len(seqs_df), 2),\n",
    "            \"other_cases\": (g[\"variant\"] == \"__OTHER__\").sum(),\n",
    "            \"avg_len\": float(np.mean(lengths)),\n",
    "            \"median_len\": float(np.median(lengths)),\n",
    "            \"variant_entropy\": float(entropy),\n",
    "            \"unique_variants\": g[\"variant\"].nunique()\n",
    "        })\n",
    "    return pd.DataFrame(rows).sort_values(\"size\", ascending=False)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 5) Supervisor-Style Wrapper Functions\n",
    "# ======================================================\n",
    "def find_num_clusters(X, k_values, cluster_algo=\"kmeans\"):\n",
    "    \"\"\"\n",
    "    Wrapper to find the best number of clusters for a given algorithm.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Embeddings.\n",
    "    k_values : iterable\n",
    "        Values of k to try.\n",
    "    cluster_algo : {'kmeans', 'som'}\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    best_result : dict\n",
    "    all_results : list of dict\n",
    "    \"\"\"\n",
    "    if cluster_algo == \"kmeans\":\n",
    "        return run_kmeans_sweep(X, k_values)\n",
    "    elif cluster_algo == \"som\":\n",
    "        return run_som_sweep(X, k_values)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported cluster_algo: {cluster_algo}\")\n",
    "\n",
    "# (Note: cluster_traces is not used in automated sweep pipeline, but added based on Williams \n",
    "#        feedback)\n",
    "def cluster_traces(X, num_clusters, cluster_algo=\"kmeans\"):\n",
    "    \"\"\"\n",
    "    -Perform clustering given vectors X and chosen number of clusters.\n",
    "    -Returns cluster labels and the fitted model (if applicable).\n",
    "    -Makes my clustering code re-usable on other datasets for those who don't want to run\n",
    "    \"sweep\" functions\n",
    "    -Allows for manual clustering when k is known\n",
    "    \"\"\"\n",
    "    if cluster_algo == \"kmeans\":\n",
    "        km = KMeans(n_clusters=num_clusters, n_init=20, random_state=GLOBAL_SEED)\n",
    "        labels = km.fit_predict(X)\n",
    "        return labels, km\n",
    "    elif cluster_algo == \"som\":\n",
    "        labels = run_som_no_bmu(X, n_clusters=num_clusters)\n",
    "        return labels, None\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported cluster_algo: {cluster_algo}\")\n",
    "\n",
    "\n",
    "def mine_from_clusters(labels, num_clusters, sequences_df):\n",
    "    \"\"\"\n",
    "    Mine cluster-level summaries from cluster labels and sequences.\n",
    "    Here we reuse the existing `cluster_summary` function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : array-like\n",
    "        Cluster labels for each trace.\n",
    "    num_clusters : int\n",
    "        Number of clusters (not strictly needed here, but kept for API consistency).\n",
    "    sequences_df : pd.DataFrame\n",
    "        DataFrame with at least 'trace_str' and 'variant' columns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Cluster summary statistics.\n",
    "    \"\"\"\n",
    "    return cluster_summary(labels, sequences_df)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 6) Run All Clustering (KMeans, SOM, HDBSCAN) using the new helpers\n",
    "# ======================================================\n",
    "K_RANGE = range(2, 16)\n",
    "\n",
    "algo_results = []\n",
    "cluster_summaries = {}\n",
    "\n",
    "all_kmeans_sweeps = {}\n",
    "all_som_sweeps = {}\n",
    "\n",
    "for enc_name, X in encoders.items():\n",
    "    print(f\"\\nüöÄ Running: {enc_name}\")\n",
    "\n",
    "    # --- KMeans ---\n",
    "    best_km, km_all = find_num_clusters(X, K_RANGE, cluster_algo=\"kmeans\")\n",
    "    all_kmeans_sweeps[enc_name] = km_all\n",
    "    filtered[f\"cluster_kmeans_{enc_name.lower()}\"] = best_km[\"labels\"]\n",
    "    cluster_summaries[(enc_name, \"KMeans\")] = mine_from_clusters(\n",
    "        best_km[\"labels\"],\n",
    "        best_km[\"k\"],\n",
    "        filtered\n",
    "    )\n",
    "\n",
    "    algo_results.append({\n",
    "        \"encoder\": enc_name,\n",
    "        \"algorithm\": \"KMeans\",\n",
    "        **{k: best_km[k] for k in [\"k\", \"silhouette\", \"ch\", \"db\"]}\n",
    "    })\n",
    "\n",
    "    # --- SOM ---\n",
    "    best_som, som_all = find_num_clusters(X, K_RANGE, cluster_algo=\"som\")\n",
    "    all_som_sweeps[enc_name] = som_all\n",
    "    filtered[f\"cluster_som_{enc_name.lower()}\"] = best_som[\"labels\"]\n",
    "    cluster_summaries[(enc_name, \"SOM\")] = mine_from_clusters(\n",
    "        best_som[\"labels\"],\n",
    "        best_som[\"k\"],\n",
    "        filtered\n",
    "    )\n",
    "\n",
    "    algo_results.append({\n",
    "        \"encoder\": enc_name,\n",
    "        \"algorithm\": \"SOM\",\n",
    "        **{k: best_som[k] for k in [\"k\", \"silhouette\", \"ch\", \"db\"]}\n",
    "    })\n",
    "\n",
    "    # --- HDBSCAN (unchanged in spirit) ---\n",
    "    hdb = run_hdbscan(X, min_cluster_size=2)\n",
    "    filtered[f\"cluster_hdbscan_{enc_name.lower()}\"] = hdb[\"labels\"]\n",
    "    cluster_summaries[(enc_name, \"HDBSCAN\")] = mine_from_clusters(\n",
    "        hdb[\"labels\"],\n",
    "        hdb[\"k\"],\n",
    "        filtered\n",
    "    )\n",
    "\n",
    "    algo_results.append({\n",
    "        \"encoder\": enc_name,\n",
    "        \"algorithm\": \"HDBSCAN\",\n",
    "        **{k: hdb[k] for k in [\"k\", \"silhouette\", \"ch\", \"db\"]}\n",
    "    })\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 7) Algorithm Comparison Table\n",
    "# ======================================================\n",
    "algo_df = pd.DataFrame(algo_results)\n",
    "print(\"\\nüìà Algorithm Comparison\")\n",
    "display(algo_df)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 8) Silhouette Trend Plotting (unchanged)\n",
    "# ======================================================\n",
    "def plot_silhouette_trend_dict(results, title):\n",
    "    k_vals = [r[\"k\"] for r in results if not np.isnan(r[\"silhouette\"])]\n",
    "    sil_vals = [r[\"silhouette\"] for r in results if not np.isnan(r[\"silhouette\"])]\n",
    "\n",
    "    if len(k_vals) == 0:\n",
    "        print(f\"No valid silhouette values for {title}.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(k_vals, sil_vals, marker='o', linewidth=2)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Silhouette\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"\\nüìà Silhouette Trends\")\n",
    "\n",
    "print(\"\\nTFIDF ‚Äî KMeans\")\n",
    "plot_silhouette_trend_dict(all_kmeans_sweeps[\"TFIDF_SVD\"], \"KMeans (TFIDF)\")\n",
    "\n",
    "print(\"\\nTFIDF ‚Äî SOM\")\n",
    "plot_silhouette_trend_dict(all_som_sweeps[\"TFIDF_SVD\"], \"SOM (TFIDF)\")\n",
    "\n",
    "print(\"\\nDOC2VEC ‚Äî KMeans\")\n",
    "plot_silhouette_trend_dict(all_kmeans_sweeps[\"DOC2VEC\"], \"KMeans (DOC2VEC)\")\n",
    "\n",
    "print(\"\\nDOC2VEC ‚Äî SOM\")\n",
    "plot_silhouette_trend_dict(all_som_sweeps[\"DOC2VEC\"], \"SOM (DOC2VEC)\")\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Skipping HDBSCAN silhouette trends (auto-detected clusters).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1204f45",
   "metadata": {},
   "source": [
    "# Process Discovery - Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9425eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# Global Process Discovery Pipeline (Annotated Version)\n",
    "# =============================================================\n",
    "# This script performs *global* process discovery using PM4Py.\n",
    "# It discovers a single process model for the entire log (no clustering),\n",
    "# evaluates conformance (fitness + precision), visualizes the resulting\n",
    "# models (Petri net / BPMN / Heuristics Net), and computes variability.\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 0) Imports\n",
    "# =============================================================\n",
    "# PM4Py evaluation\n",
    "from pm4py.algo.evaluation.precision import algorithm as precision_evaluator\n",
    "from pm4py.algo.evaluation.replay_fitness.algorithm import Variants as FitnessVariants\n",
    "from pm4py.algo.evaluation.precision.algorithm import Variants as PrecisionVariants\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Log conversion utilities\n",
    "from pm4py.objects.log.util import dataframe_utils\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "\n",
    "# Fitness (token-based replay) and precision\n",
    "from pm4py.algo.evaluation.replay_fitness.variants import token_replay as fitness_evaluator\n",
    "from pm4py.algo.evaluation.precision import algorithm as precision_evaluator\n",
    "from pm4py.objects.log.util import dataframe_utils\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.algo.evaluation.replay_fitness.variants import token_replay as fitness_evaluator\n",
    "from pm4py.algo.evaluation.precision import algorithm as precision_evaluator\n",
    "\n",
    "from pm4py.visualization.heuristics_net import visualizer as hn_visualizer\n",
    "\n",
    "# =============================================================\n",
    "# 1) Configuration\n",
    "# =============================================================\n",
    "MINER_TYPE = \"inductive\"  # options: \"inductive\", \"heuristics\", \"alpha\"\n",
    "RANDOM_STATE = 42         # deterministic sampling\n",
    "EVAL_SAMPLE_SIZE = 2000   # conformance evaluation log sample size\n",
    "\n",
    "# Conformance settings\n",
    "# checks *fit* (does the model reproduce behavior?)\n",
    "FITNESS_VARIANT = FitnessVariants.TOKEN_BASED\n",
    "# checks *specificity* (does it avoid overgeneralizing?)\n",
    "PRECISION_VARIANT = PrecisionVariants.ETCONFORMANCE_TOKEN\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0e526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def discover_model_for_miner(log):\n",
    "    \"\"\"\n",
    "    Wrapper to call the correct PM4Py discovery algorithm based on MINER_TYPE.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    (net, im, fm, heu_net, process_tree)\n",
    "    - heuristics miner returns heu_net instead\n",
    "    - inductive miner returns process tree\n",
    "    \"\"\"\n",
    "    if MINER_TYPE == \"inductive\":\n",
    "        process_tree = inductive_miner.apply(log)\n",
    "        net, im, fm = pt_converter.apply(process_tree)\n",
    "        return net, im, fm, None, process_tree     # heuristics net = None\n",
    "\n",
    "    elif MINER_TYPE == \"alpha\":\n",
    "        net, im, fm = alpha_miner.apply(log)\n",
    "        return net, im, fm, None, None     # heuristics net = None\n",
    "\n",
    "    elif MINER_TYPE == \"heuristics\":\n",
    "        heu_net = heuristics_miner.apply_heu(log)\n",
    "        net, im, fm = hn_converter.apply(heu_net)\n",
    "        return net, im, fm, heu_net, None\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"‚ùå Unknown MINER_TYPE: {MINER_TYPE}\")\n",
    "\n",
    "# =============================================================\n",
    "# 3) Sampling Utility\n",
    "# =============================================================\n",
    "def maybe_sample_log(event_log, max_traces):\n",
    "    \"\"\"Sample a subset of traces from a log if it exceeds max_traces.\n",
    "\n",
    "    Args:\n",
    "        event_log (EventLog): PM4Py event log object.\n",
    "        max_traces (int): Maximum number of traces to keep.\n",
    "\n",
    "    Returns:\n",
    "        EventLog: A sampled or original log.\n",
    "    \"\"\"\n",
    "    if (max_traces is None) or (len(event_log) <= max_traces):\n",
    "        return event_log\n",
    "    idx = np.random.RandomState(RANDOM_STATE).choice(len(event_log), size=max_traces, replace=False)\n",
    "    return EventLog([event_log[i] for i in sorted(idx)])\n",
    "\n",
    "# =============================================================\n",
    "# 4) Variability Measure\n",
    "# =============================================================\n",
    "# Variant variability reflects behavioral diversity.\n",
    "def compute_variability_ratio(log):\n",
    "    \"\"\"\n",
    "    Compute variability ratio = (# unique variants) / (# total traces).\n",
    "    Lower values indicate more homogeneous clusters.\n",
    "\n",
    "    Args:\n",
    "        log (EventLog): PM4Py EventLog for the cluster.\n",
    "\n",
    "    Returns:\n",
    "        float: variability ratio\n",
    "    \"\"\"\n",
    "    if len(log) == 0:\n",
    "        return 0.0\n",
    "    variant_set = set()\n",
    "    for trace in log:\n",
    "        seq = tuple(e[\"concept:name\"] for e in trace)\n",
    "        variant_set.add(seq)\n",
    "    return len(variant_set) / len(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac038dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section MUST be ran before Process discovery\n",
    "from pm4py.algo.evaluation.replay_fitness import algorithm as fitness_evaluator\n",
    "from pm4py.algo.evaluation.precision import algorithm as precision_evaluator\n",
    "\n",
    "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
    "from pm4py.objects.conversion.process_tree import converter as pt_converter\n",
    "\n",
    "from pm4py.algo.discovery.heuristics import algorithm as heuristics_miner\n",
    "from pm4py.objects.conversion.heuristics_net import converter as hn_converter\n",
    "\n",
    "from pm4py.algo.evaluation.replay_fitness.algorithm import Variants as FitnessVariants\n",
    "from pm4py.algo.evaluation.precision.algorithm import Variants as PrecisionVariants\n",
    "\n",
    "from pm4py.visualization.petri_net import visualizer as pn_visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d5af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.visualization.petri_net import visualizer as pn_visualizer\n",
    "from pm4py.objects.conversion.process_tree import converter as process_tree_converter\n",
    "from pm4py.visualization.bpmn import visualizer as bpmn_visualizer\n",
    "\n",
    "# =============================================================\n",
    "# 5) Global Process Discovery + Evaluation\n",
    "# =============================================================\n",
    "def evaluate_global_model(df, metrics_df=None):\n",
    "    \"\"\"\n",
    "    Perform discovery ‚Üí visualization ‚Üí conformance evaluation for a *global* model.\n",
    "\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): Raw event log with columns:\n",
    "    - case:concept:name\n",
    "    - concept:name\n",
    "    - time:timestamp\n",
    "\n",
    "\n",
    "    metrics_df (pd.DataFrame or None):\n",
    "    ‚Äì If provided, append new results.\n",
    "    ‚Äì If None, create the table.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Updated metrics table.\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # Convert pandas ‚Üí PM4Py EventLog\n",
    "    # ----------------------------------------------\n",
    "    params = {log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: \"case:concept:name\"}\n",
    "    global_log = log_converter.apply(df, variant=log_converter.Variants.TO_EVENT_LOG, parameters=params)\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # Model discovery\n",
    "    # ----------------------------------------------\n",
    "    net, im, fm, heu_net, process_tree = discover_model_for_miner(global_log)\n",
    "    \n",
    "    # ----------------------------------------------\n",
    "    # Visualization (Petri net, BPMN, or heuristics net)\n",
    "    # ----------------------------------------------\n",
    "   # --- Visualization settings ---\n",
    "    if MINER_TYPE == \"inductive\":\n",
    "        VISUALIZATION_THRESHOLD = None  # set to None = no size limit\n",
    "\n",
    "        #from pm4py.visualization.petri_net import visualizer as pn_visualizer\n",
    "\n",
    "        print(\"PN size:\", len(net.places), len(net.transitions))\n",
    "\n",
    "        if VISUALIZATION_THRESHOLD is None or len(net.transitions) < VISUALIZATION_THRESHOLD:\n",
    "            #print(\"Rendering PN...\")\n",
    "            #gviz = pn_visualizer.apply(net, im, fm)\n",
    "            #pn_visualizer.view(gviz)\n",
    "            print(\"Rendering BPMN...\")\n",
    "            bpmn_graph = process_tree_converter.apply(\n",
    "            process_tree,\n",
    "            variant=process_tree_converter.Variants.TO_BPMN)\n",
    "            \n",
    "            gviz = bpmn_visualizer.apply(bpmn_graph)\n",
    "            bpmn_visualizer.view(gviz)\n",
    "        else:\n",
    "            print(f\"PN too large ({len(net.transitions)} transitions) ‚Äî skipping visualization\")\n",
    "            \n",
    "    elif MINER_TYPE == \"heuristics\":\n",
    "        # heuristics nets need their own visualizer\n",
    "        \n",
    "        print(\"Rendering Heuristics Net...\")\n",
    "        gviz = hn_visualizer.apply(heu_net)\n",
    "        hn_visualizer.view(gviz)\n",
    "    # --- END ADDED BLOCK ---\n",
    "    \n",
    "    # ----------------------------------------------\n",
    "    # Conformance checking\n",
    "    # ----------------------------------------------\n",
    "    # maybe sample\n",
    "    eval_log = maybe_sample_log(global_log, EVAL_SAMPLE_SIZE)\n",
    "\n",
    "    # conformance eval\n",
    "    fit_res  = fitness_evaluator.apply(eval_log, net, im, fm, variant=FITNESS_VARIANT)\n",
    "    fitness  = fit_res.get(\"average_trace_fitness\", fit_res.get(\"perc_fit_traces\", np.nan))\n",
    "    precision = precision_evaluator.apply(eval_log, net, im, fm, variant=PRECISION_VARIANT)\n",
    "    fscore    = 2 * (precision * fitness) / (precision + fitness) if (precision + fitness) > 0 else 0.0\n",
    "\n",
    "    # variability\n",
    "    global_variability = compute_variability_ratio(global_log)\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # Build result row\n",
    "    # ----------------------------------------------\n",
    "    row_dict = {\n",
    "        \"Method\": \"Global\",\n",
    "        \"Cluster\": \"Global\",\n",
    "        \"NumTraces\": len(global_log),\n",
    "        \"Precision\": float(precision),\n",
    "        \"Fitness\": float(fitness),\n",
    "        \"FScore\": float(fscore),\n",
    "        \"VariabilityRatio\": float(global_variability)\n",
    "    }\n",
    "\n",
    "    # Initialize or append\n",
    "    if (metrics_df is None) or (len(metrics_df) == 0):\n",
    "        metrics_df = pd.DataFrame([row_dict])\n",
    "    else:\n",
    "        # subsequent calls\n",
    "        metrics_df = pd.concat([metrics_df, pd.DataFrame([row_dict])], ignore_index=True)\n",
    "\n",
    "    print(\n",
    "        f\"üåê Global Model ({MINER_TYPE}) ‚Üí \"\n",
    "        f\"Precision: {precision:.3f}, Fitness: {fitness:.3f}, \"\n",
    "        f\"F-score: {fscore:.3f}, Variability Ratio: {global_variability:.3f}\"\n",
    "    )\n",
    "\n",
    "    return metrics_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2baff58",
   "metadata": {},
   "source": [
    "## When MINER_TYPE = \"heuristics\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525b6ccd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "metrics_df = evaluate_global_model(df)\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df54abd7",
   "metadata": {},
   "source": [
    "## When MINER_TYPE = \"inductive\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc4bdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics_df = evaluate_global_model(df)\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14962e89",
   "metadata": {},
   "source": [
    "# Process Discovery - Per Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f1c232",
   "metadata": {},
   "source": [
    "## Inductive Miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# Per‚ÄëCluster Process Discovery & Evaluation (Annotated Version)\n",
    "# =============================================================\n",
    "# This script mirrors the global process discovery workflow but applied\n",
    "# separately to each cluster. For every cluster, we:\n",
    "# 1. Extract all traces belonging to that cluster\n",
    "# 2. Convert them to an EventLog\n",
    "# 3. Discover a process model (IMf version of Inductive Miner)\n",
    "# 4. Optionally visualize the model (BPMN)\n",
    "# 5. Compute conformance (precision, fitness, F-score)\n",
    "# 6. Compute variability ratio\n",
    "# 7. Append results to a cluster‚Äëlevel metrics table\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# 0) Discovery Function ‚Äî IMf Variant\n",
    "# =============================================================\n",
    "# IMf = Inductive Miner (infrequent) ‚Äî a *more flexible* configuration.\n",
    "# It captures more behavioral detail (higher fitness) at the cost of more\n",
    "# complex / less generalizable models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcadb379",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---BALANCED-----\n",
    "# Keeps most traces\n",
    "# Removes some infrequent paths\n",
    "# Produces a reasonably interpretable model\n",
    "\n",
    "# --- NEW: IMf variant specifically for per-cluster discovery ---\n",
    "def discover_model_for_miner_imf(log):\n",
    "    \"\"\"\n",
    "    IMf version ‚Äî matches old pipeline behavior.\n",
    "    \"\"\"\n",
    "    tree = inductive_miner.apply(\n",
    "        log,\n",
    "        variant=inductive_miner.Variants.IMf,\n",
    "        parameters={\"noise_threshold\": 0.2}\n",
    "    )\n",
    "    net, im, fm = pt_converter.apply(tree)\n",
    "    return net, im, fm, tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485b554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---STRICT(simpler models, less fitness)-----\n",
    "# Filters weak paths aggressively\n",
    "# Great for large noisy logs\n",
    "# Model may underfit (i.e. lose rare but valid behavior)\n",
    "#def discover_model_for_miner_imf(log):\n",
    "    \"\"\"\n",
    "    IMf version ‚Äî matches old pipeline behavior.\n",
    "    \"\"\"\n",
    "    #tree = inductive_miner.apply(\n",
    "        #log,\n",
    "        #variant=inductive_miner.Variants.IMf,\n",
    "        #parameters={\n",
    "            #\"noise_threshold\": 0.4,\n",
    "            #\"min_dfg_occurrences\": 2\n",
    "        #}\n",
    "    #)\n",
    "    #net, im, fm = pt_converter.apply(tree)\n",
    "    #return net, im, fm, tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453fd7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---FLEXIBLE(complex model, higher fitness)-----\n",
    "# Almost no pruning\n",
    "# captures most behavior\n",
    "# Model may overfit(i.e. messy and less generalizable)\n",
    "#def discover_model_for_miner_imf(log):\n",
    "    \"\"\"\n",
    "    IMf version ‚Äî matches old pipeline behavior.\n",
    "    \"\"\"\n",
    "    #tree = inductive_miner.apply(\n",
    "        #log,\n",
    "        #variant=inductive_miner.Variants.IMf,\n",
    "        #parameters={\n",
    "            #\"noise_threshold\": 0.1,\n",
    "            #\"min_dfg_occurrences\": 1\n",
    "        #}\n",
    "    #)\n",
    "    #net, im, fm = pt_converter.apply(tree)\n",
    "    #return net, im, fm, tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0279a135",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================\n",
    "# 1) Per‚ÄëCluster Discovery & Evaluation\n",
    "# =============================================================\n",
    "\n",
    "def discover_and_evaluate_per_cluster(\n",
    "    df: pd.DataFrame,\n",
    "    filtered: pd.DataFrame,\n",
    "    cluster_col: str,\n",
    "    method_name: str = None,\n",
    "    cluster_metrics_df: pd.DataFrame = None,   # ‚Üê renamed\n",
    "    skip_noise: bool = True,\n",
    "    noise_label: int = -1,\n",
    "    visualize: bool = False,\n",
    "    outdir: str = \"cluster_models\",\n",
    "    sample_size: int = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Discover and evaluate process models *independently for each cluster*.\n",
    "\n",
    "\n",
    "    Args:\n",
    "    df: Original full event log (pandas DataFrame).\n",
    "    filtered: DataFrame containing case IDs + cluster assignments.\n",
    "    cluster_col: Column name inside `filtered` identifying cluster labels.\n",
    "    method_name: Name to insert in the metrics output (defaults to cluster_col).\n",
    "    cluster_metrics_df: Existing table to append results to.\n",
    "    skip_noise: If True, skip the noise cluster (e.g., HDBSCAN label -1).\n",
    "    noise_label: Integer label representing noise.\n",
    "    visualize: If True, render BPMN for each cluster.\n",
    "    outdir: Directory for saving models (not used here but reserved).\n",
    "    sample_size: Max traces to evaluate (None ‚Üí default = EVAL_SAMPLE_SIZE).\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    Updated cluster_metrics_df containing one row per cluster.\n",
    "    \"\"\"\n",
    "    method_name = method_name or cluster_col\n",
    "    #sample_size = sample_size if sample_size is not None else EVAL_SAMPLE_SIZE\n",
    "    \n",
    "    # ----------------------------------------------------------\n",
    "    # Validate index structure (cases must be index or a column)\n",
    "    # ----------------------------------------------------------\n",
    "    if filtered.index.name != \"case:concept:name\":\n",
    "        if \"case:concept:name\" in filtered.columns:\n",
    "            filtered = filtered.set_index(\"case:concept:name\", drop=True)\n",
    "        else:\n",
    "            raise ValueError(\"`filtered` must have case ids on the index or a 'case:concept:name' column.\")\n",
    "\n",
    "    if cluster_col not in filtered.columns:\n",
    "        raise ValueError(f\"`{cluster_col}` not found in filtered columns: {list(filtered.columns)}\")\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Iterate over clusters\n",
    "    # ----------------------------------------------------------\n",
    "    for c, case_ids in filtered.groupby(cluster_col).groups.items():\n",
    "        if skip_noise and c == noise_label:\n",
    "            continue\n",
    "\n",
    "        cluster_df = df[df[\"case:concept:name\"].isin(case_ids)].copy()\n",
    "        if cluster_df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Extract cluster-specific subset of df\n",
    "        cluster_df = dataframe_utils.convert_timestamp_columns_in_df(cluster_df)\n",
    "        \n",
    "        # Standard timestamp normalization for PM4Py\n",
    "        cluster_df = cluster_df.sort_values(\n",
    "            [\"case:concept:name\",\"time:timestamp\"],\n",
    "            ignore_index=True\n",
    "        )\n",
    "        # Mapping for PM4Py conversion\n",
    "        params = {\n",
    "            \"case_id\":       \"case:concept:name\",\n",
    "            \"activity_key\":  \"concept:name\",\n",
    "            \"timestamp_key\": \"time:timestamp\",\n",
    "        }\n",
    "        \n",
    "        \n",
    "\n",
    "        log = log_converter.apply(cluster_df, variant=log_converter.Variants.TO_EVENT_LOG, parameters=params)\n",
    "\n",
    "        # ------------------------------------------------------\n",
    "        # Discover model (IMf variant for detailed structure)\n",
    "        # ------------------------------------------------------\n",
    "        net, im, fm, tree = discover_model_for_miner_imf(log)\n",
    "\n",
    "        eval_log = log\n",
    "        \n",
    "        # ------------------------------------------------------\n",
    "        # Conformance metrics\n",
    "        # ------------------------------------------------------\n",
    "        fit_res  = fitness_evaluator.apply(eval_log, net, im, fm, variant=FITNESS_VARIANT)\n",
    "        fitness  = fit_res.get(\"log_fitness\", None)\n",
    "\n",
    "        precision = precision_evaluator.apply(eval_log, net, im, fm, variant=PRECISION_VARIANT)\n",
    "        \n",
    "        if precision is not None and fitness is not None and (precision+fitness) > 0:\n",
    "            fscore    = (2 * precision * fitness / (precision + fitness))  \n",
    "        else: \n",
    "            fscore = 0.0\n",
    "\n",
    "        variability = compute_variability_ratio(log)\n",
    "        \n",
    "        # ------------------------------------------------------\n",
    "        # Append metrics row\n",
    "        # ------------------------------------------------------\n",
    "\n",
    "        row_dict = {\n",
    "            \n",
    "            \"Method\":           method_name,\n",
    "            \"Cluster\":          f\"Cluster {c}\",\n",
    "            \"NumTraces\":        len(log),\n",
    "            \"Precision\":        float(precision),\n",
    "            \"Fitness\":          float(fitness),\n",
    "            \"FScore\":           float(fscore),\n",
    "            \"VariabilityRatio\": float(variability),\n",
    "            \"Miner\":            \"inductive\"\n",
    "        }\n",
    "\n",
    "        # ‚Üê saving to the cluster-level dataframe\n",
    "        if (cluster_metrics_df is None) or (len(cluster_metrics_df) == 0):\n",
    "            cluster_metrics_df = pd.DataFrame([row_dict])\n",
    "        else:\n",
    "            cluster_metrics_df = pd.concat(\n",
    "                [cluster_metrics_df, pd.DataFrame([row_dict])],\n",
    "                ignore_index=True\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            f\"‚úÖ {cluster_col} / Cluster {c} (IMf) ‚Üí \"\n",
    "            f\"Prec {precision:.3f} | Fit {fitness:.3f} | F1 {fscore:.3f} | Var {variability:.3f} | \"\n",
    "            f\"Traces {len(log)}\"\n",
    "        )\n",
    "        \n",
    "        # ------------------------------------------------------\n",
    "        # Optional visualization\n",
    "        # ------------------------------------------------------\n",
    "\n",
    "        if visualize:\n",
    "            #print(f\"Rendering PN for cluster {c}...\")\n",
    "            #gviz = pn_visualizer.apply(net, im, fm)\n",
    "            #pn_visualizer.view(gviz)\n",
    "            print(f\"Rendering BPMN for cluster {c}...\")\n",
    "            bpmn_graph = process_tree_converter.apply(\n",
    "                            tree,\n",
    "                            variant=process_tree_converter.Variants.TO_BPMN\n",
    "            )\n",
    "            gviz = bpmn_visualizer.apply(bpmn_graph)\n",
    "            bpmn_visualizer.view(gviz)\n",
    "\n",
    "    return cluster_metrics_df    # important for downstream analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30301c5a",
   "metadata": {},
   "source": [
    "### TFIDF Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd95317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kmeans + TFIDF  (FIRST)\n",
    "cluster_metrics_df = discover_and_evaluate_per_cluster(\n",
    "    df=df,\n",
    "    filtered=filtered,\n",
    "    cluster_col=\"cluster_kmeans_tfidf_svd\",\n",
    "    method_name=\"KMeans-TFIDF\",\n",
    "    cluster_metrics_df=None,          # ‚Üê When set to none, it resets the dataframe cluster_metrics_df\n",
    "    skip_noise=True,\n",
    "    visualize=True\n",
    ")\n",
    "\n",
    "# SOM + TFIDF\n",
    "cluster_metrics_df = discover_and_evaluate_per_cluster(\n",
    "    df=df,\n",
    "    filtered=filtered,\n",
    "    cluster_col=\"cluster_som_tfidf_svd\",\n",
    "    method_name=\"SOM-TFIDF\",\n",
    "    cluster_metrics_df=cluster_metrics_df,  # << append, not None\n",
    "    skip_noise=True,\n",
    "    visualize=True\n",
    ")\n",
    "\n",
    "# HDBSCAN + TFIDF\n",
    "cluster_metrics_df = discover_and_evaluate_per_cluster(\n",
    "    df=df,\n",
    "    filtered=filtered,\n",
    "    cluster_col=\"cluster_hdbscan_tfidf_svd\",\n",
    "    method_name=\"HDBSCAN-TFIDF\",\n",
    "    cluster_metrics_df=cluster_metrics_df,  # << append, not None\n",
    "    skip_noise=True,\n",
    "    noise_label=-1,\n",
    "    visualize=True\n",
    ")\n",
    "\n",
    "display(cluster_metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898870f8",
   "metadata": {},
   "source": [
    "### Doc2Vec Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e865d27",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# KMeans + Doc2Vec\n",
    "cluster_metrics_df = discover_and_evaluate_per_cluster(\n",
    "    df=df,\n",
    "    filtered=filtered,\n",
    "    cluster_col=\"cluster_kmeans_doc2vec\",\n",
    "    method_name=\"KMeans-Doc2Vec\",\n",
    "    cluster_metrics_df=cluster_metrics_df,     # first call in this block\n",
    "    skip_noise=True,\n",
    "    visualize=True\n",
    ")\n",
    "\n",
    "# SOM + Doc2Vec\n",
    "cluster_metrics_df = discover_and_evaluate_per_cluster(\n",
    "    df=df,\n",
    "    filtered=filtered,\n",
    "    cluster_col=\"cluster_som_doc2vec\",\n",
    "    method_name=\"SOM-Doc2Vec\",\n",
    "    cluster_metrics_df=cluster_metrics_df,   # append to same df\n",
    "    skip_noise=True,\n",
    "    visualize=True\n",
    ")\n",
    "\n",
    "# HDBSCAN + Doc2Vec\n",
    "cluster_metrics_df = discover_and_evaluate_per_cluster(\n",
    "    df=df,\n",
    "    filtered=filtered,\n",
    "    cluster_col=\"cluster_hdbscan_doc2vec\",\n",
    "    method_name=\"HDBSCAN-Doc2Vec\",\n",
    "    cluster_metrics_df=cluster_metrics_df,   # append again\n",
    "    skip_noise=True,\n",
    "    noise_label=-1,\n",
    "    visualize=True\n",
    ")\n",
    "\n",
    "\n",
    "display(cluster_metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1c5358",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_metrics_df.to_csv(\"cluster_metrics_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88009c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_only = cluster_metrics_df[cluster_metrics_df[\"Method\"].str.contains(\"TFIDF\")]\n",
    "\n",
    "# Weighted averages of conformance metrics\n",
    "# Should a cluster with 5 cases influence overall analysis as much as a cluster with 400 cases? (in this case, no)\n",
    "weights = tfidf_only[\"NumTraces\"]\n",
    "\n",
    "weighted_avg_tfidf = (tfidf_only[[\"Precision\", \"Fitness\", \"FScore\"]]\n",
    "                      .multiply(weights, axis=0)\n",
    "                      .sum() / weights.sum())\n",
    "\n",
    "weighted_avg_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bc5b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only TFIDF-based methods\n",
    "tfidf_only = cluster_metrics_df[cluster_metrics_df[\"Method\"].str.contains(\"TFIDF\")]\n",
    "\n",
    "# Weighted averages per method (TFIDF only)\n",
    "method_weighted_tfidf = (\n",
    "    tfidf_only\n",
    "    .groupby(\"Method\")\n",
    "    .apply(lambda g: (\n",
    "        (g[[\"Precision\",\"Fitness\",\"FScore\"]] * g[\"NumTraces\"].values[:,None]).sum()\n",
    "        / g[\"NumTraces\"].sum()\n",
    "    ))\n",
    ")\n",
    "\n",
    "method_weighted_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1ce2f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TextVec2 Env",
   "language": "python",
   "name": "textvec2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
